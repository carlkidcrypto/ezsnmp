name: Integration Tests (Docker)

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  check-changes:
    runs-on: ubuntu-latest
    outputs:
      any_changed: ${{ steps.changed-files.outputs.any_changed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v5

      - name: Check for changes in source code
        id: changed-files
        uses: tj-actions/changed-files@e0021407031f5be11a464abee9a0776171c79891 # v47.0.1
        with:
          files: |
            .github/workflows/integration_tests_docker.yml
            docker/**
            ezsnmp/**/*.cpp
            ezsnmp/**/*.h
            integration_tests/**
            python_tests/snmpd.conf
            setup.cfg
            setup.py

  integration-tests-docker:
    needs: check-changes
    if: needs.check-changes.outputs.any_changed == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    permissions:
      pull-requests: write
    strategy:
      fail-fast: false
      matrix:
        distribution: [almalinux10_netsnmp_5.9, archlinux_netsnmp_5.7, archlinux_netsnmp_5.8, archlinux_netsnmp_5.9, centos7_netsnmp_5.7, rockylinux8_netsnmp_5.8, rockylinux9_netsnmp_5.9]
    steps:
      - name: Checkout repository
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v5

      - name: Compute Docker image cache key
        id: cache-key
        run: |
          WEEK_KEY=$(date -u +"%Y-%V")
          echo "week=$WEEK_KEY" >> $GITHUB_OUTPUT

      - name: Restore cached Docker image tar
        id: cache-restore
        uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
        with:
          path: .docker-image-cache/${{ matrix.distribution }}.tar
          key: docker-image-${{ matrix.distribution }}-${{ steps.cache-key.outputs.week }}-v1
          restore-keys: |
            docker-image-${{ matrix.distribution }}-

      - name: Load Docker image from cache (if present)
        run: |
          set -euo pipefail
          TAR_PATH=".docker-image-cache/${{ matrix.distribution }}.tar"
          if [ -f "$TAR_PATH" ]; then
            echo "Loading cached image from $TAR_PATH"
            docker load -i "$TAR_PATH"
          else
            echo "No cached image tar found. Will pull from registry."
          fi

      - name: Pull Docker image (refresh from registry)
        run: |
          set -euo pipefail
          IMAGE="carlkidcrypto/ezsnmp_test_images:${{ matrix.distribution }}-latest"
          docker pull "$IMAGE"

      - name: Save Docker image to cache tar
        if: always()
        run: |
          set -euo pipefail
          mkdir -p .docker-image-cache
          IMAGE="carlkidcrypto/ezsnmp_test_images:${{ matrix.distribution }}-latest"
          TAR_PATH=".docker-image-cache/${{ matrix.distribution }}.tar"
          docker save "$IMAGE" -o "$TAR_PATH"

      - name: Start container and SNMP daemon
        run: |
          set -euo pipefail
          IMAGE="carlkidcrypto/ezsnmp_test_images:${{ matrix.distribution }}-latest"
          CONTAINER_NAME="${{ matrix.distribution }}_integration_container"
          ENTRY_SCRIPT="/usr/local/bin/DockerEntry.sh"
          HOST_SOURCE_PATH="$PWD"
          CONTAINER_WORK_DIR="/ezsnmp"
          echo ">>> Starting container: $CONTAINER_NAME"
          echo "    - Using host source path: $HOST_SOURCE_PATH"
          echo "    - Mounting to: $CONTAINER_WORK_DIR"
          docker rm -f "$CONTAINER_NAME" >/dev/null 2>&1 || true
          if ! docker run -d \
            --name "$CONTAINER_NAME" \
            -v "$HOST_SOURCE_PATH:$CONTAINER_WORK_DIR" \
            "$IMAGE" \
            /bin/bash -c "$ENTRY_SCRIPT false & tail -f /dev/null"; then
              echo "ERROR: Docker run failed for $IMAGE" >&2
              exit 1
          fi
          echo ">>> Waiting for SNMP daemon to start..."
          sleep 10

      - name: Build and install ezsnmp in container
        run: |
          set -euo pipefail
          CONTAINER_NAME="${{ matrix.distribution }}_integration_container"
          echo ">>> Building and installing ezsnmp inside container"
          docker exec -t "$CONTAINER_NAME" bash -c '
            set -e
            export PATH=/usr/local/bin:/opt/rh/gcc-toolset-11/root/usr/bin:/opt/rh/gcc-toolset-13/root/usr/bin:/opt/rh/devtoolset-11/root/usr/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64:${LD_LIBRARY_PATH:-}
            cd /ezsnmp
            echo "    - Installing ezsnmp"
            python3 -m pip install . --quiet
            echo "    - Build complete"
          '

      - name: Run integration tests
        run: |
          set -euo pipefail
          CONTAINER_NAME="${{ matrix.distribution }}_integration_container"
          echo ">>> Running integration tests inside container: $CONTAINER_NAME"
          docker exec -t "$CONTAINER_NAME" bash -c '
            set -e
            export PATH=/usr/local/bin:/opt/rh/gcc-toolset-11/root/usr/bin:/opt/rh/gcc-toolset-13/root/usr/bin:/opt/rh/devtoolset-11/root/usr/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64:${LD_LIBRARY_PATH:-}
            cd /ezsnmp/integration_tests
            echo "    - Starting integration tests"
            if ./run_integration_tests.sh > /ezsnmp/integration_tests_full.log 2>&1; then
              echo "    - Integration tests completed successfully"
            else
              echo "    - Integration tests reported failures (continuing to collect artifacts)"
            fi
            exit 0
          '

      - name: Collect artifacts from container
        if: always()
        run: |
          set -euo pipefail
          CONTAINER_NAME="${{ matrix.distribution }}_integration_container"
          mkdir -p integration_results

          # Copy the main log file
          if docker exec "$CONTAINER_NAME" test -f /ezsnmp/integration_tests_full.log; then
            docker cp "$CONTAINER_NAME:/ezsnmp/integration_tests_full.log" \
              integration_results/integration_tests_${{ matrix.distribution }}.log

            # Extract key metrics into per-file logs (matching native workflow behavior)
            cd integration_results
            LOG_FILE="integration_tests_${{ matrix.distribution }}.log"

            grep "Total execution time:" "$LOG_FILE" > total_execution_time_${{ matrix.distribution }}.log || true
            grep "usm_unknown_security_name_counter:" "$LOG_FILE" > usm_unknown_security_name_counter_${{ matrix.distribution }}.log || true
            grep "connection_error_counter:" "$LOG_FILE" > connection_error_counter_${{ matrix.distribution }}.log || true

            # Try to extract test_file_descriptors from the results directory
            OUT_DIR=$(grep -m1 "Results saved to:" "$LOG_FILE" | sed 's/Results saved to: //' | tr -d '\r') || true
            if [ -n "$OUT_DIR" ]; then
              # OUT_DIR is a path inside the container, try to copy from container
              docker cp "$CONTAINER_NAME:$OUT_DIR/test_file_descriptors.log" test_file_descriptors_${{ matrix.distribution }}.log 2>/dev/null || true
            fi

            # Fallback: grep relevant lines if file wasn't copied
            if [ ! -f "test_file_descriptors_${{ matrix.distribution }}.log" ]; then
              grep -A2 "Running file descriptor tests" "$LOG_FILE" > test_file_descriptors_${{ matrix.distribution }}.log || true
            fi

            cd ..
          fi

          # Find and copy the test_results directory
          RESULTS_DIR=$(docker exec "$CONTAINER_NAME" bash -c 'ls -dt /ezsnmp/integration_tests/test_results_* 2>/dev/null | head -1' || echo "")
          if [ -n "$RESULTS_DIR" ] && [ "$RESULTS_DIR" != "" ]; then
            echo "Found results directory: $RESULTS_DIR"
            docker cp "$CONTAINER_NAME:$RESULTS_DIR" integration_results/test_results_${{ matrix.distribution }}/ || true
          else
            echo "No test_results directory found"
          fi

      - name: Extract test metrics
        if: always()
        run: |
          set -euo pipefail
          LOG_FILE="integration_results/integration_tests_${{ matrix.distribution }}.log"
          if [ -f "$LOG_FILE" ]; then
            echo "=== Test Summary for ${{ matrix.distribution }} ==="
            grep -E "(PASS:|FAIL:|COMPLETED:|Total execution time:)" "$LOG_FILE" || echo "No test summary found"
            echo ""
            echo "=== Checking for errors ==="
            grep -i "error\|fail\|segfault\|signal 11" "$LOG_FILE" | head -20 || echo "No critical errors found"
          fi

      - name: Stop and remove container
        if: always()
        run: |
          docker rm -f "${{ matrix.distribution }}_integration_container" >/dev/null 2>&1 || true

      - name: Upload Integration Test Results
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v5
        with:
          name: integration-test-results-docker_${{ matrix.distribution }}
          path: |
            integration_results/**

  summary:
    needs: integration-tests-docker
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          pattern: integration-test-results-docker_*
          merge-multiple: true

      - name: Generate test summary
        run: |
          python3 -c "
          import os
          import re
          import statistics
          
          # Parse all log files
          data = {}
          distributions = []
          
          for filename in os.listdir('.'):
              if filename.startswith('integration_tests_') and filename.endswith('.log'):
                  distro = filename.replace('integration_tests_', '').replace('.log', '')
                  distributions.append(distro)
                  
                  with open(filename, 'r') as f:
                      lines = f.readlines()
                  
                  for line in lines:
                      # Match patterns like: multi_process_snmp_get: - 2 - Total execution time: 1.234 seconds
                      match = re.search(r'(\w+): - (\d+) - (.+): (.+)', line.strip())
                      if match:
                          test_type, num, field, value = match.groups()
                          num = int(num)
                          try:
                              val = float(value.split()[0])
                          except (ValueError, IndexError):
                              continue
                          key = (distro, test_type, num, field)
                          if key not in data:
                              data[key] = []
                          data[key].append(val)
          
          # Generate summary with comment marker for PR comments
          summary = '<!-- integration-test-docker-summary -->\\n## Integration Tests (Docker) Results \\u23f1\\ufe0f\\n\\n'
          
          if not data:
              summary += 'No test results found.\\n'
          else:
              # Group by distribution
              for distro in sorted(set(d for d, _, _, _ in data.keys())):
                  distro_data = {k: v for k, v in data.items() if k[0] == distro}
                  if not distro_data:
                      continue
                  
                  summary += f'### {distro.replace(\"_\", \" \").title()}\\n\\n'
                  summary += '| Test Type | Workers | Field | Min (s) | Max (s) | Avg (s) | StdDev \\U0001f4ca |\\n'
                  summary += '|-----------|---------|-------|---------|---------|---------|----------|\\n'
                  
                  for key in sorted(distro_data.keys()):
                      _, test_type, num, field = key
                      values = distro_data[key]
                      
                      # Format test type to be more readable
                      test_name = test_type.replace('_', ' ').title()
                      
                      if len(values) > 1:
                          min_v = min(values)
                          max_v = max(values)
                          avg_v = statistics.mean(values)
                          std_v = statistics.stdev(values)
                          summary += f'| {test_name} | {num} | {field} | {min_v:.3f} | {max_v:.3f} | {avg_v:.3f} | {std_v:.3f} |\\n'
                      elif len(values) == 1:
                          summary += f'| {test_name} | {num} | {field} | {values[0]:.3f} | {values[0]:.3f} | {values[0]:.3f} | N/A |\\n'
                  
                  summary += '\\n'

          # Add file descriptor test results
          summary += '### File Descriptor Tests \\U0001f4c1\\n\\n'
          fd_data = {}
          for filename in os.listdir('.'):
              if filename.startswith('test_file_descriptors_') and filename.endswith('.log'):
                  with open(filename, 'r') as f:
                      content = f.read()

                  distro = filename.replace('test_file_descriptors_', '').replace('.log', '')

                  # Extract session type results
                  for line in content.split('\\n'):
                      # Parse FD counts: Subprocess PID Open FDs before/after
                      fd_before_match = re.search(r'Subprocess PID: Open FDs before: (\\d+) \\[(.+?)\\]', line)
                      fd_after_match = re.search(r'Subprocess PID Open FDs after: (\\d+) \\[(.+?)\\]', line)
                      # Parse execution times
                      time_match = re.search(r'work_(\\w+) \\[(.+?)\\]: Total execution time: ([\\d.]+)', line)
                      # Parse average times per call
                      avg_match = re.search(r'Average time per SNMP get call \\((.+?)\\) \\[(.+?)\\]: ([\\d.]+)', line)

                      if fd_before_match:
                          count, sess = fd_before_match.groups()
                          key = (distro, sess)
                          if key not in fd_data:
                              fd_data[key] = {}
                          if 'fd_before' not in fd_data[key]:
                              fd_data[key]['fd_before'] = []
                          fd_data[key]['fd_before'].append(int(count))

                      if fd_after_match:
                          count, sess = fd_after_match.groups()
                          key = (distro, sess)
                          if key not in fd_data:
                              fd_data[key] = {}
                          if 'fd_after' not in fd_data[key]:
                              fd_data[key]['fd_after'] = []
                          fd_data[key]['fd_after'].append(int(count))

                      if time_match:
                          mode, sess, time_val = time_match.groups()
                          key = (distro, sess, mode)
                          if key not in fd_data:
                              fd_data[key] = {}
                          if 'exec_time' not in fd_data[key]:
                              fd_data[key]['exec_time'] = []
                          fd_data[key]['exec_time'].append(float(time_val))

                      if avg_match:
                          mode, sess, avg_val = avg_match.groups()
                          key = (distro, sess, mode.replace(' ', '_'))
                          if key not in fd_data:
                              fd_data[key] = {}
                          if 'avg_time' not in fd_data[key]:
                              fd_data[key]['avg_time'] = []
                          fd_data[key]['avg_time'].append(float(avg_val))

          if fd_data:
              # Group by distribution
              for distro in sorted(set(k[0] for k in fd_data.keys() if isinstance(k, tuple) and len(k) >= 1)):
                  summary += f'#### {distro.replace(\"_\", \" \").title()}\\n\\n'
                  summary += '| Session Type | Mode | FD Before | FD After | FD Leak | Exec Time (s) | Avg/Call (s) |\\n'
                  summary += '|--------------|------|-----------|----------|---------|---------------|--------------|\\n'

                  # Get all keys for this distribution
                  distro_keys = [k for k in fd_data.keys() if isinstance(k, tuple) and len(k) >= 3 and k[0] == distro]

                  for key in sorted(distro_keys):
                      d, sess, mode = key[:3]

                      # Find corresponding FD data
                      fd_key = (d, sess)
                      fd_before = fd_data.get(fd_key, {}).get('fd_before', [])
                      fd_after = fd_data.get(fd_key, {}).get('fd_after', [])
                      exec_times = fd_data[key].get('exec_time', [])
                      avg_times = fd_data[key].get('avg_time', [])

                      if exec_times or avg_times:
                          mode_display = 'no close' if mode == 'get_no_close' or mode == 'no_close' else 'close'
                          sess_display = sess.replace('SESS_', '').replace('_ARGS', '')

                          avg_exec = statistics.mean(exec_times) if exec_times else 0
                          avg_avg = statistics.mean(avg_times) if avg_times else 0

                          # Calculate FD leak (after - before)
                          if fd_before and fd_after:
                              avg_before = statistics.mean(fd_before)
                              avg_after = statistics.mean(fd_after)
                              fd_leak = avg_after - avg_before
                              summary += f'| {sess_display} | {mode_display} | {avg_before:.0f} | {avg_after:.0f} | {fd_leak:+.0f} | {avg_exec:.3f} | {avg_avg:.6f} |\\n'
                          else:
                              summary += f'| {sess_display} | {mode_display} | - | - | - | {avg_exec:.3f} | {avg_avg:.6f} |\\n'

                  summary += '\\n'

              summary += '\\u2705 File descriptor tests completed successfully\\n'
          else:
              summary += '\\u274c No file descriptor test results found\\n'

          # Write to GitHub Step Summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(summary)

          # Also save to file for PR comment
          with open('integration_test_docker_summary.md', 'w') as f:
              f.write(summary)

          print('Summary generated successfully')
          "
          echo "Test summary generated"

      - name: Post PR comment with integration-test summary
        if: github.event_name == 'pull_request'
        id: find-integration-comment
        uses: peter-evans/find-comment@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: 'github-actions[bot]'
          body-includes: '<!-- integration-test-docker-summary -->'

      - name: Update PR comment with integration-test summary
        if: github.event_name == 'pull_request'
        uses: peter-evans/create-or-update-comment@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue-number: ${{ github.event.pull_request.number }}
          comment-id: ${{ steps.find-integration-comment.outputs.comment-id }}
          body-file: integration_test_docker_summary.md
          edit-mode: replace
          comment-tag: integration-test-docker-summary
